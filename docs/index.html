<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="MIMIC-D: Multi-modal Imitation for MultI-agent Coordination with Decentralized Diffusion Policies">
  <meta property="og:title" content="MIMIC-D"/>
  <meta property="og:description" content="Multi-modal Imitation for MultI-agent Coordination with Decentralized Diffusion Policies"/>
  <meta property="og:url" content="https://iconlab.negarmehr.com/MIMIC-D/"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <!-- <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/> -->


  <meta name="twitter:title" content="MIMIC-D">
  <meta name="twitter:description" content="Multi-modal Imitation for MultI-agent Coordination with Decentralized Diffusion Policies">
  <!-- Path to banner image, should be in the path listed below. Optimal dimensions are 1200X600-->
  <!-- <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image"> -->
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Multi-Robot, Decentralized, Coordination, Robot Learning, Diffusion Models, Robotics">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>MIMIC-D</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({ tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]} });
	  MathJax.Hub.Config({ TeX: { equationNumbers: {autoNumber: "AMS"} } });
  </script>
  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>

  <style>
	.reference {
	   margin-bottom: 3mm;
	}
  </style>
</head>
<body>


<!--div style="float:left;"-->
<div class="logo-left">
  <a href="https://www.berkeley.edu/" target="_blank">
    <img src="static/images/UC-Berkeley-Seal.png" alt="UC Berkeley" />
  </a>
</div>

<div class="logo-right">
  <a href="https://iconlab.negarmehr.com/" target="_blank">
    <img src="static/images/ICON_Lab.png" alt="ICON Lab" />
  </a>
</div>



  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">MIMIC-D: Multi-modal Imitation for MultI-agent Coordination with Decentralized Diffusion Policies</h1>
						<div class="is-size-5 publication-authors"> <!-- Paper authors -->
              <span class="author-block"><a href="https://dayiethandong.com/" target="_blank">Dayi Dong</a>,</span>
	      			<span class="author-block"><a href="https://maulikbhatt.web.illinois.edu" target="_blank">Maulik Bhatt</a>,</span>
              <span class="author-block"><a href="https://sites.google.com/yonsei.ac.kr/seoyeon-choi-home/" target="_blank">Seoyeon Choi</a>,</span>
              <span class="author-block"><a href="https://negarmehr.com/" target="_blank">Negar Mehr</a></span><br>
							<!-- <a href="https://www.roboticsproceedings.org/rss21/p078.pdf" target="_blank">Robotics: Science and Systems (RSS) 2025</a> -->
            </div>

							<div class="column has-text-centered">
								<div class="publication-links">
										 <!-- Arxiv PDF link -->
									<span class="link-block">
										<a href="https://arxiv.org/pdf/2509.14159" target="_blank"
										class="external-link button is-normal is-rounded is-dark">
										<span class="icon">
											<i class="fas fa-file-pdf"></i>
										</span>
										<span>Paper</span>
									</a>
								</span>

							<!-- Github link -->
							<span class="link-block">
								<a href="https://github.com/labicon/MIMIC-D?tab=readme-ov-file" target="_blank"
								class="external-link button is-normal is-rounded is-dark">
								<span class="icon">
									<i class="fab fa-github"></i>
								</span>
								<span>Code</span>
							</a>
						</span>

						<!-- ArXiv abstract Link -->
						<span class="link-block">
							<a href="https://arxiv.org/abs/2509.14159" target="_blank"
							class="external-link button is-normal is-rounded is-dark">
							<span class="icon">
								<i class="ai ai-arxiv"></i>
							</span>
							<span>arXiv</span>
						</a>
					</span>

<!-- Blog Link -->
						<!--span class="link-block">
							<a href="https://jean-baptistebouvier.github.io/assets/blog_posts/policed_RL/" target="_blank"
							class="external-link button is-normal is-rounded is-dark">
							<span class="icon">
		<i class="fas fa-scroll"></i>
							</span>
							<span>Blog</span>
						</a>
</span-->
			    
            </div>
					</div>
        </div>
      </div>
    </div>
  </div>
</section>


	
<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop width="852" height="480">
        <source src="videos/Video_Submission_ICRA_2026.mp4" type="video/mp4">
      </video>
			<div class="columns is-centered has-text-centered">
				<div class="column is-full">
		      <div class="content has-text-justified">
		        An overview of MIMIC-D: Multi-modal Imitation for MultI-agent Coordination with Decentralized Diffusion Policies. <br>
					</div>
				</div>
			</div>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Overview -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
			<div class="column is-full">
				<h2 class="title is-3">Overview</h2>
				<div class="content has-text-justified">
					<ul>
						<li><strong>MIMIC-D</strong> is a Centralized Training Decentralized Execution (CTDE) framework for multi-agent coordination using only <strong>local observations</strong> that effectively captures multi-modality in the expert data.</li>
						<li> During the centralized training, agents <strong>learns to coordinate</strong> with each other via access to centralized loss function. The agents preserve coordination during receding horizon decentralized execution. </li>
						<li> We demonstrate the effectiveness of MIMIC-D in multiple simulated domains and on a complicated bimanual hardware setup, showing significant improvements over baselines in recovering expert trajectory distributions while reducing collisions and task failures </li>
					</ul>
					<div class="has-text-centered" style="margin-top: 1.5rem;">
						<img src="pictures/fig1_v5.1.jpg" alt="MIMIC-D Overview" style="max-height:400px; display:block; margin:auto;">
					</div>
				</div>
			</div>
    </div>
  </div>
</section>


<section class="section hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
			<div class="column is-full">
				<h2 class="title is-3">Challenge</h2>
				<div class="content has-text-justified">
					<ul>
						<li>Achieving <strong>coordination</strong> in multi-agent systems is challenging, especially in the presence of multi-modality</li>
						<li>For example, when two
							people are walking toward each other head-on, they can avoid
							a collision if they both choose to yield right or both choose
							to yield left. Both strategies are acceptable, but the agents
							need to decide together to achieve the desired <strong>coordination</strong>.</li>
						<li>One popular way to learn motion policies is <strong>Imitation Learning</strong> </li>
						<li>However, most existing imitation learning methods are not designed to handle multi-modality and coordination in multi-agent systems.</li>
						<li>We assume that we have access to a dataset $\mathcal{D}$ containing $M$ expert demonstrations where each demonstration is a collection of $N$ agents interacting with each other for a finite horizon of time $T$.</li>
						<li>Each demonstration in the dataset is a set of tuples $\{(\xi^i,o^i)\}_{i=1}^N$, where $o^i$ is the observation of agent $i$ and $\xi^i = \{a^i_0,\ldots,a^i_{T-1}\}$ is the corresponding finite-horizon ($T$) trajectory of actions executed by agent $i$ associated with observation $o^i$.</li>
						<li>Note that the expert demonstrations may be multi-modal in nature, i.e., for a given observation $o^i$, we may have two or more different expert actions.</li>
						<li>Our goal is to learn a set of decentralized policies $\{\pi_{\theta^1}^1, \ldots, \pi_{\theta^N}^N\}$ that can collectively reproduce individual agent behavior and learn implicit coordination among agents from the dataset $\mathcal{D}$.</li>	
					</ul>
				</div>
			</div>
		</div>
  </div>
</section>


<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-3">Our Key Idea</h2>

				<div class="content has-text-justified">
        	Our key idea is to use <strong>diffusion models</strong> to capture multi-modality in expert demonstrations.
        </div>
        <!-- <img src="pictures/DDAT_scheme.svg" alt="DDAT illustration" style="height:200px !important; display:block !important; margin:auto !important;"/>
				
        <div class="content has-text-justified">
          <ul>
						<li>We propose to <strong>project</strong> the trajectories generated by our diffusion model to make them admissible during both training and inference.</li>
						<li>Trajectory projections are <strong>auto-regressive</strong> since we need to reach first $s_1$, then $s_2$ and so on.</li>
						<li>Since $f$ is a black-box, we cannot compute reachable sets and instead sample a <strong>polytopic under-approximation</strong> of the reachable sets as shown on the video. </li>
          </ul>
        </div>

				<video poster="" id="tree" autoplay controls muted loop width="1920" height="1080">
        	<source src="videos/Projection_animation.mp4" type="video/mp4">
				</video>
				 -->
      </div>
    </div>
  </div>
</section>


<section class="section hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
			<div class="column is-full">
				<h2 class="title is-3">Our Approach</h2>
				<div class="content has-text-justified">
					We employ a CTDE paradigm for learning multi-modal multi-agent coordination policies using diffusion models. 
					<ul>
						<li>For our MIMIC-D method, we model decentralized policies $\pi^i$ as a conditional diffusion model with denoiser network $D_{\theta^i}(\xi^i;\sigma, o^i)$.</li>
						<li>For each agent, $D_{\theta^i}$ takes three inputs, the noisy action trajectory $\xi^i_k$, current noise level $\sigma_k$, and observation $o^i$,
							and with iterative denoising produces a sampled action trajectory ${\xi}^i_K$.</li>
						<li>Centralized Training: we jointly train the policies for all the agents in the system. During the training, the agents' denoising policies share a single loss and have access to all local observations. This is necessary to promote coordination and collision-avoidant behaviors. The joint loss function: $$ \mathcal{L}_{\text{total}}(\theta) = \sum_{i=1}^N \mathcal{L}^i_{\text{diff}}(\theta^i) $$
						</li>
						<li>Decentralized Execution: At every timestep, each agent $i$ individually acquires their local observation $o^i$ from the environment. Then, the agents sample their own policy $\pi^i_{\theta^i}$ for ${\xi}^i$, which has horizon $T$. Only the first $h$ ($h < T$) steps of each agent's actions are executed.
						</li>
					</ul>
	      </div>
		  <!-- Architecture Image -->
		  <div class="has-text-centered" style="margin-top: 1.5rem;">
			<img src="pictures/architecture_v12.2.jpg" alt="MIMIC-D Architecture" style="max-height:400px; display:block; margin:auto;">
		  </div>
			</div>
    </div>
  </div>
</section>

<section class="section hero is-light">
	<div class="container is-max-desktop">
	  <div class="columns is-centered has-text-centered">
		<div class="column is-four-fifths">
		  <h2 class="title is-3">Simulation Experiments</h2>
				  <div class="content has-text-justified">
					  We employed MIMIC-D on three different simulation scenariors:
					  <ul>
						  <li><strong>Two-Agent Swap</strong>: Two agents are trying to swap positions with one another while avoiding a central obstacle. This environment involves six possible modes as shown below.</li>
						  <img src="pictures/swap_env_demo_2.png" alt="Swap Demo" style="height:200px !important; display:block !important; margin:auto !important;"/>
						  <li><strong>Three-Agent Road Crossing</strong>: Three agents are trying to avoid one another while on their way to their respective goal locations. As shown below, this can be achieved in enumerous ways.</li>
						  <img src="pictures/road_env_demo_2.png" alt="Road Demo" style="height:200px !important; display:block !important; margin:auto !important;"/>
						  <li><strong>Two-Arm Lift</strong>: The two Kinova3 arms collaborate to lift the pot and transfer it to the other side while avoiding the obstacle (red box). This challenging high dimensional task has two modes on which both the agents need to coordinate.</li>
						  <div class="columns is-centered" style="margin-top: 1rem;">
							<div class="column is-half has-text-centered">
							  <video autoplay loop muted playsinline style="max-height:250px;">
								<source src="videos/two_arm_lift_sim_left.mp4" type="video/mp4">
								Your browser does not support the video tag.
							  </video>
							</div>
							<div class="column is-half has-text-centered">
							  <video autoplay loop muted playsinline style="max-height:250px;">
								<source src="videos/two_arm_lift_sim_right.mp4" type="video/mp4">
								Your browser does not support the video tag.
							  </video>
							</div>
						  </div>
						</ul>
				  </div>

				  <div class="content has-text-justified">
					  The results of these simulation results are summarized in the following plots. As it can be seen MIMIC-D achieves higher coordination amongst agents and significantly less collisions and task failures.
					  <ul>
						  <!-- <li><strong>Two-Agent Swap</strong>: Two agents are trying to swap positions with one another while avoiding a central obstacle. This environment involves six possible modes.</li>
						  <img src="pictures/swap_env_demo_2.png" alt="Swap Demo" style="height:200px !important; display:block !important; margin:auto !important;"/>
						  <li><strong>Three-Agent Road Crossing</strong>: Three agents are trying to avoid one another while on their way to their respective goal locations. </li>
						  <img src="pictures/road_env_demo_2.png" alt="Road Demo" style="height:200px !important; display:block !important; margin:auto !important;"/>
						  <li><strong>Two-Arm Lift</strong>: The two Kinova3 arms collaborate to lift the pot and transfer it to the other side while avoiding the obstacle (red box).</li> -->
						  <div class="columns is-centered" style="margin-top: 1rem;">
							<div class="column is-half has-text-centered">
							  <img src="pictures/swap_collisions.png" alt="Swap Collisions" style="height:200px !important; display:block !important; margin:auto !important;"/>
							</div>
							<div class="column is-half has-text-centered">
							  <img src="pictures/road_collisions.png" alt="Road Collisions" style="height:200px !important; display:block !important; margin:auto !important;"/>
							</div>
							<div class="column is-half has-text-centered">
							  <img src="pictures/two_arm_sim_results.png" alt="Two Arm Simulation Results" style="height:200px !important; display:block !important; margin:auto !important;"/>
							</div>
						  </div>
						</ul>
				  </div>
		</div>
	  </div>
	</div>
  </section>

  <section class="section hero is-small">
	<div class="container is-max-desktop">
	  <div class="columns is-centered has-text-centered">
		<div class="column is-four-fifths">
		  <h2 class="title is-3">Hardware Experiments</h2>
		  <div class="content has-text-justified">
			To demonstrate MIMIC-D in high-dimensional real hardware, we also perform the Two-Arm Lift task in the real world.
		  </div>
  
		  <div class="columns is-centered" style="margin-top: 1rem;">
			<div class="column is-half has-text-centered">
			  <video autoplay loop muted playsinline style="max-height:250px;">
				<source src="videos/left_mode_hardware_16x.mp4" type="video/mp4">
				Your browser does not support the video tag.
			  </video>
			</div>
			<div class="column is-half has-text-centered">
			  <video autoplay loop muted playsinline style="max-height:250px;">
				<source src="videos/right_mode_hardware_16x.mp4" type="video/mp4">
				Your browser does not support the video tag.
			  </video>
			</div>
		  </div>
  
		</div>
	  </div>
	</div>
  </section>
  
	

  
<!-- Image carousel -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Projections make trajectories admissible and better</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item">
	        <img src="pictures/Hopper_SAE_500.svg" alt="Statewise admissibility error" style="height:400px !important; display:block !important; margin:auto !important;"/>
        	<h6 class="subtitle has-text-centered" style="font-size: 16px;">
						Statewise admissibility error over 500 Hopper trajectories. <br>
						The error <strong><span style="color:blue;">without projections</span></strong> is much larger than when using <br>
						<strong><span style="color:DarkOrange;">projections at inference</span></strong> or
						<strong><span style="color:green;">training with projections $\mathcal{P}^\text{ref}$</span></strong>. <br>
						All diffusion models generate only state trajectories.
					</h6>
	      </div>
	      <div class="item">
        	<img src="pictures/Hopper_CAE_500.svg" alt="Cumulative admissibility error" style="height:400px !important; display:block !important; margin:auto !important;"/>
        	<h6 class="subtitle has-text-centered" style="font-size: 16px;">
						Cumulative admissibility error over 500 Hopper trajectories. <br>
						The error <strong><span style="color:blue;">without projections</span></strong> is much larger than when using <br>
						<strong><span style="color:DarkOrange;">projections at inference</span></strong> or
						<strong><span style="color:green;">training with projections $\mathcal{P}^\text{ref}$</span></strong>. <br>
						All diffusion models generate only state trajectories.
		      </h6>
	      </div>
	      <div class="item">
	        <img src="pictures/Hopper_falling_ratio_500.svg" alt="Falling ratios" style="height:400px !important; display:block !important; margin:auto !important;"/>
	        <h5 class="subtitle has-text-centered" style="font-size: 16px;">
						Ratios of open-loop Hopper trajectories having fallen at a given timestep. <br>
						The trajectories <strong><span style="color:blue;">without projections</span></strong> and those with
						<strong><span style="color:DarkOrange;">projections at inference</span></strong> <br>
						start to fall significantly earlier than those
						<strong><span style="color:green;">training with projections $\mathcal{P}^\text{ref}$</span></strong>. <br>
						All diffusion models generate only state trajectories.
					</h5>
      	</div>
	     	<div class="item">
	     		<img src="pictures/Walker_SAE_400.svg" alt="Statewise admissibility error" style="height:400px !important; display:block !important; margin:auto !important;"/>
	      	<h6 class="subtitle has-text-centered" style="font-size: 14px !important;">
						Statewise admissibility error over 400 Walker trajectories. <br>
						The error <strong><span style="color:blue;">without projections</span></strong> is much larger than when using
						<strong><span style="color:DarkOrange;">projections at inference</span></strong>. <br>
						Our model <strong><span style="color:green;">trained with projections $\mathcal{P}^\text{SA}$</span></strong> has no error. <br>
						All diffusion models generate states and actions.
					</h6>
		    </div>
		    <div class="item">
		    	<img src="pictures/Walker_CAE_400.svg" alt="Cumulative admissibility error" style="height:400px !important; display:block !important; margin:auto !important;"/>
				 	<h6 class="subtitle has-text-centered" style="font-size: 16px;">
						Cumulative admissibility error over 400 Walker trajectories. <br>
						The error <strong><span style="color:blue;">without projections</span></strong> is much larger than when using
						<strong><span style="color:DarkOrange;">projections at inference</span></strong>. <br>
						Our model <strong><span style="color:green;">trained with projections $\mathcal{P}^\text{SA}$</span></strong> has no error. <br>
						All diffusion models generate states and actions.
		      </h6>
		    </div>
		    <div class="item">
	      <img src="pictures/Walker_falling_ratio_400.svg" alt="Falling ratios" style="height:400px !important; display:block !important; margin:auto !important;"/>
	        <h6 class="subtitle has-text-centered" style="font-size: 16px;">
						Ratios of open-loop Walker trajectories having fallen at a given timestep. <br>
						The trajectories <strong><span style="color:blue;">without projections</span></strong> and those with
						<strong><span style="color:DarkOrange;">projections at inference</span></strong> <br>
							start to fall significantly earlier than those
						<strong><span style="color:green;">training with projections $\mathcal{P}^\text{SA}$</span></strong>. <br>
						All diffusion models generate states and actions.
	      	</h6>
	      </div>
			</div>
	  </div>
	</div>
</section> -->
<!-- End image carousel -->




<!-- <section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop width="337" height="600" style="margin: auto;!important">
        <source src="videos/GO2_comparison.mp4" type="video/mp4">
      </video>
      <div class="columns is-centered has-text-centered">
				<div class="column is-full">
		      <div class="content has-text-justified">
		        Unitree GO2 <strong>open-loop</strong> trajectories tasked with going straight. <br>
						Both models without projections deviate from walking straight, <br>
						whereas our DDAT model follows prefectly the command.
					</div>
				</div>
      </div>
    </div>
  </div>
</section> -->



	

<!-- Image carousel -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
	    <h2 class="title is-3">Projections should only occur at low noise levels</h2>
			<div class="content has-text-justified">
				<p>
					All diffusion models generate state-action trajectories with projections <strong>starting</strong> from either the
					<strong><span style="color:blue;">beginning</span></strong> of inference, i.e., projecting at all noise levels, or <strong>starting</strong>
					<strong><span style="color:green;">mid</span></strong>-inference, or project only once
					<strong><span style="color:DarkOrange;">after</span></strong> inference.
				</p>
			</div>
	    
	    <div id="results-carousel" class="carousel results-carousel">
	      <div class="item">
	      	<img src="pictures/Hopper_SA_Q2_SAE_500.svg" alt="Statewise admissibility error" style="height:400px !important; display:block !important; margin:auto !important;"/>
			   	<h6 class="subtitle has-text-centered" style="font-size: 16px;">
						Statewise admissibility error over 500 Hopper trajectories. <br>
						The admissibility error is similar between models no matter when projections start, <br>
						because they all perform a projection at the end of inference.
					</h6>
	      </div>
	      <div class="item">
	        <img src="pictures/Hopper_SA_Q2_falling_ratio_500.svg" alt="Falling ratios" style="height:400px !important; display:block !important; margin:auto !important;"/>
		      <h6 class="subtitle has-text-centered" style="font-size: 16px;">
						Ratios of open-loop Hopper trajectories having fallen at a given timestep. <br>
						The trajectories projecting at <strong><span style="color:blue;">high</span></strong> noise levels fall significantly earlier than those
						projecting at <strong><span style="color:green;">small</span></strong> and 
						<strong><span style="color:DarkOrange;">zero</span></strong> noise levels.
					</h6>
				</div>
	      <div class="item">
			    <img src="pictures/Quad_100trajs_Q2.svg" alt="Statewise admissibility error" style="height:400px !important; display:block !important; margin:auto !important;"/>
					<h6 class="subtitle has-text-centered" style="font-size: 16px;">
						Quadcopter trajectories tasked with slaloming between <strong><span style="color:red;">obstacles</span></strong> following the dashed line. <br>
						Trajectories generated with projections at <strong><span style="color:blue;">high</span></strong> noise levels are incapable of performing the task, <br>
						while trajectories with projections at <strong><span style="color:green;">small</span></strong> and
						<strong><span style="color:DarkOrange;">zero</span></strong> noise levels succeed.
					</h6>
  			</div>
			</div>
		</div>
	</div>
</section> -->


<!-- Presentation video-->
<!--section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
        <h2 class="title is-3">Video Presentation given at RSS 2024</h2>
          <div class="publication-video">
            <iframe width="560" height="315" src="https://www.youtube.com/embed/xMWSqjRrcVc?si=Puj4cTJtLFcLGyH1" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"   referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
        </div>
        </div>
      </div>
    </div>
  </div>
</section-->
	
<!-- Youtube video -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <!- - Paper video. - ->
      <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video">
            <!- - Youtube embed code here - ->
            <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End youtube video -->


<!-- Video carousel -->
<!-- <section class="hero is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Generating quadcopter trajectories</h2>
			<div class="content has-text-justified">
				<p>
		      Objective: slalom between <strong><span style="color:red;">obstacles</span></strong> to reach the <strong><span style="color:green;">target</span></strong>.
		    </p>
		  </div>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop width="1280" height="720">
            <source src="videos/S_sampled_ID_above.mp4" type="video/mp4">
					</video>
					<h6 class="subtitle has-text-centered" style="font-size: 16px;">
						Model generating state trajectories without projections. <br>
						The sampled trajectory would succeed if it was feasible, <br>
						but <strong><span style="color:DarkOrange;">inverse dynamics</span></strong> shows how far off the trajectory actually is.
     			</h6>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop width="1280" height="720">
            <source src="videos/S_inference_ID_above.mp4" type="video/mp4">
          </video>
					<h6 class="subtitle has-text-centered" style="font-size: 16px;">
						Model generating state trajectories with projections only at inference. <br>
						The sampled trajectory is close to feasible, but not sufficiently to<br>
						prevent the <strong><span style="color:DarkOrange;">inverse dynamics</span></strong> from crashing.
      		</h6>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop width="1280" height="720">\
            <source src="videos/S_ref_proj_above.mp4" type="video/mp4">
          </video>
					<h6 class="subtitle has-text-centered" style="font-size: 16px;">
					  Model generating state trajectories trained with reference projections. <br>
					  The sampled trajectory is very close to the <strong><span style="color:DarkOrange;">inverse dynamics</span></strong> and both succeed.
	      	</h6>
        </div>
			  <div class="item item-video4">
          <video poster="" id="video4" autoplay controls muted loop width="1280" height="720">\
            <source src="videos/S_ref_proj_track.mp4" type="video/mp4">
          </video>
					<h6 class="subtitle has-text-centered" style="font-size: 16px;">
						Model generating state trajectories trained with reference projections. <br>
						The sampled trajectory slaloms between the obstacles and reaches the target.
     			</h6>
        </div>
		 		<div class="item item-video5">
          <video poster="" id="video5" autoplay controls muted loop width="1280" height="720">
            <source src="videos/SA_sampled_ol_above.mp4" type="video/mp4">
					</video>
					<h6 class="subtitle has-text-centered" style="font-size: 16px;">
						Model generating states and actions without projections. <br>
						The sampled trajectory would succeed if it was feasible, <br>
						but the <strong><span style="color:DarkOrange;">open-loop</span></strong> shows how far off the trajectory actually is.
					</h6>
        </div>
        <div class="item item-video6">
          <video poster="" id="video6" autoplay controls muted loop width="1280" height="720">
            <source src="videos/SA_inference_proj_above.mp4" type="video/mp4">
          </video>
					<h6 class="subtitle has-text-centered" style="font-size: 16px;">
						Model generating states and actions with projections only at inference. <br>
						The sampled trajectory flies through the obstacle,<br>
						while the <strong><span style="color:DarkOrange;">open-loop</span></strong> diverges.
					</h6>
        </div>
        <div class="item item-video7">
          <video poster="" id="video7" autoplay controls muted loop width="1280" height="720">\
            <source src="videos/SA_proj_sampled_ol_above.mp4" type="video/mp4">
          </video>
					<h6 class="subtitle has-text-centered" style="font-size: 16px;">
						Model generating states and actions trained with SA-projections. <br>
						The sampled trajectory is admissible as it matches its <br>
						<strong><span style="color:DarkOrange;">open-loop</span></strong> realisation and both succeed.
	     	  </h6>
        </div>
				<div class="item item-video8">
          <video poster="" id="video8" autoplay controls muted loop width="1280" height="720">\
            <source src="videos/SA_proj_ol_track.mp4" type="video/mp4">
          </video>
					<h6 class="subtitle has-text-centered" style="font-size: 16px;">
					  Model generating states and actions trained with reference projections. <br>
					  The sampled trajectory slaloms between the obstacles and reaches the target.
					</h6>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End video carousel -->




<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop width="360" height="450" style="margin: auto;!important">
        <source src="videos/Hoppers_comparison.mp4" type="video/mp4">
      </video>
      <div class="columns is-centered has-text-centered">
				<div class="column is-full">
		      <div class="content has-text-justified">
        		MuJoCo Hopper <strong>open-loop</strong> trajectories. <br>
						Without projections the Hopper fall earlier than with our projections.
					</div>
				</div>
      </div>
    </div>
  </div>
</section> -->


<!-- Paper poster -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>
      <iframe  src="poster.pdf" width="100%" height="1100"></iframe>
      </div>
    </div>
  </section> -->
<!--End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{dong2025mimic,
  title={MIMIC-D: Multi-modal Imitation for MultI-agent Coordination with Decentralized Diffusion Policies},
  author={Dong, Dayi and Bhatt, Maulik and Choi, Seoyeon and Mehr, Negar},
  journal={arXiv preprint arXiv:2509.14159},
  year={2025}
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


	
	
<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <!-- <p>
            This work was supported by the Google BAIR Commons Project and the National Science Foundation, under grants DGE-2146752, ECCS-2145134, CAREER Award, CNS-2423130, and CCF-2423131. 
          </p> -->
          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
